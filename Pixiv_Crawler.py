import re
import os
import os.path as op
import sys
import json
import threading
import colorama
from colorama import init,Fore,Back,Style
from New_api import *
from Downloader import *
from RSA import *

init(autoreset=True)

class Pixiv(object):
    def __init__(self):
        self.root = os.getcwd()
        self.cache_path = "{}\\cache\\".format(self.root)
        if not op.exists(self.cache_path):
            os.makedirs(self.cache_path)

        self.User_Info = self.Load_List("User_Info")
        if not self.User_Info:
            self.User_Info = {"User_Name": "", "Password": "", "Collect_Progress": 0, "Download_Path": "", "Upadte_Mode": True}
        self.USER_ID = ""
        self.USER_NAME = ""      
        self.api = New_api()
        self.RSA = rsacrypt(self.root)
        self.Log_in()
            
    def Log_in(self):
        if not self.User_Info["User_Name"]:
            self.User_Info["User_Name"] = input("è¯·è¾“å…¥ç”¨æˆ·åï¼š")
        if not self.User_Info["Password"]:    
            #å¯¹è¾“å…¥çš„å¯†ç è¿›è¡ŒåŠ å¯†
            self.User_Info["Password"] = self.RSA.encrypt(input("è¯·è¾“å…¥å¯†ç ï¼š"))
        self.api.require_appapi_hosts()
        self.api.set_accept_language('zh-CN')
        Login_Messgae = self.api.login(self.User_Info["User_Name"], self.RSA.decrypt(self.User_Info["Password"])).response
        self.USER_ID = Login_Messgae.user.id
        self.USER_NAME = Login_Messgae.user.name
        print("Welcome Back {}".format(self.USER_NAME))
        
    #ä»Žæœ¬åœ°è¯»å–jsonæ–‡ä»¶
    def Load_List(self, File_Name):
        path = "{}{}_cache".format(self.cache_path, File_Name)
        if op.isfile(path):
            try:
                with open(path, 'r') as Cache_File:
                    L = json.load(Cache_File)
            except:
                # ç¼“å­˜æ–‡ä»¶æ ¼å¼å‡ºçŽ°é—®é¢˜ï¼Œåˆ é™¤ç¼“å­˜æ–‡ä»¶
                os.remove(path)
                return []
            else:              
                return L
        else:
            return []
         
    #ä¿å­˜æŒ‡å®šå˜é‡åˆ°jsonæ–‡ä»¶
    def Save_List(self, File_Name, Target_List):
        with open("{}{}_cache".format(self.cache_path, File_Name), 'w') as Cache_File:      
            json.dump(Target_List, Cache_File, cls = MyEncoder)
        
    def Get_Follow_List(self):  
        # å¦‚æžœå­˜åœ¨ç¼“å­˜æ–‡ä»¶ï¼Œåˆ™ä»Žæœ¬åœ°è¯»å–è¯¥ç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨
        Follow_List = self.Load_List("{}_Follow_List".format(self.USER_NAME))
        # å‘Apiè¯·æ±‚å…³æ³¨äººæ•°
        Total_Follow = self.api.user_detail(self.USER_ID).profile.total_follow_users
        # å¦‚æžœå…³æ³¨äººæ•°æœªå¢žåŠ ï¼Œä¸æŠ“å–æ–°åˆ—è¡¨
        if Total_Follow == len(Follow_List):
            print("æˆåŠŸä»Žç¼“å­˜ä¸­è¿›è¡ŒåŠ è½½, ä½ å…³æ³¨äº† \033[33m{}\033[0m åä½œè€…ã€‚\n".format(len(Follow_List)))
            return
        elif op.isfile("{}{}_Follow_List_cache.json".format(self.cache_path, self.USER_NAME)):
                os.remove("{}{}_Follow_List_cache.json".format(self.cache_path, self.USER_NAME))
        Follow_List = []
        # å‘Apiè¯·æ±‚å…³æ³¨æ¸…å•
        json_result = self.api.user_following(self.USER_ID)
        while True:
            for users in json_result.user_previews:
                ID = users.user.id
                Name = self.Filter_Name(users.user.name)
                Follow_List.append([ID, Name])
            self.Save_List("{}_Follow_List".format(self.USER_NAME), Follow_List)
            print("\rå·²å…³æ³¨çš„ä½œè€…åˆ—è¡¨æ­£åœ¨æ”¶é›†ä¸­, è¿›åº¦ï¼š\033[33m{}/{}\033[0m".format(len(Follow_List),Total_Follow), end="")
            # å¦‚æžœå½“å‰é¡µé¢ä¸ºæœ€åŽä¸€é¡µåˆ™é€€å‡ºå¾ªçŽ¯
            if not json_result.next_url:
                break
            # è¯·æ±‚æ¸…å•ä¸‹ä¸€é¡µ
            json_result = self.api.next_page_f(json_result.next_url)

        self.User_Info["Collect_Progress"] = 0
        self.Save_List("User_Info", crawler.User_Info)
        print("\rå·²å…³æ³¨çš„ä½œè€…åˆ—è¡¨æ”¶é›†å®Œæˆ, ä½ å…³æ³¨äº† \033[33m{}\033[0m åä½œè€…ã€‚\n".format(len(Follow_List)))            

    def Collect_Illust_Info(self, Illust_Info, ID = ""):
        if ID:
            Illust_Info = self.api.illust_detail(ID).illust        
        # æå–å‡ºæœ‰ç”¨ä¿¡æ¯å­˜å…¥å­—å…¸
        Illust_Dict = {"ID": str(Illust_Info.id), "Title": Illust_Info.title,"Illust_Date": "".join(list(filter(str.isdigit, Illust_Info.create_date)))[:8],
                       "Caption": Illust_Info.caption[:50], "Tags": Illust_Info.tags,
                       "Ugoira": True if Illust_Info.type == "ugoira" else False, 
                       "Muti_Page": True if Illust_Info.page_count > 1 else False, "is_block": False,
                       "Image_Urls": Illust_Info.meta_single_page["original_image_url"] if Illust_Info.page_count == 1 else Illust_Info.meta_pages
                       }
        # å¯¹åˆæ­¥æå–å‡ºçš„ä¿¡æ¯è¿›è¡Œç²¾ç®€åŒ–ï¼Œå¹¶è¿›è¡Œåˆ†æžï¼Œè¿‡æ»¤æŽ‰ä¸å¿…ä¸‹è½½çš„å†…å®¹
        Illust_Dict = self.Filter(Illust_Dict)
        return Illust_Dict       

    def Filter(self, Illust_Dict):
        Tag = Illust_Dict["Tags"]
        if not Tag:    
            Illust_Dict["is_block"] = True
            return Illust_Dict

        try:
            Tag[0]["name"]
        except:
            pass
        else:
            for i in range(len(Tag)):
                if Tag[i]["translated_name"] is not None:
                    Tag[i] = Tag[i]["translated_name"]
                else:
                    Tag[i] = Tag[i]["name"]

            if Illust_Dict["Muti_Page"]:
                for i in range(len(Illust_Dict["Image_Urls"])):
                    Illust_Dict["Image_Urls"][i] = Illust_Dict["Image_Urls"][i]["image_urls"]["original"]
            elif Illust_Dict["Ugoira"]:
                Ugoira_Data = self.api.ugoira_metadata(Illust_Dict["ID"]).ugoira_metadata
                Illust_Dict["Image_Urls"] = [Ugoira_Data.zip_urls["medium"], Ugoira_Data.frames[0]["delay"]]

        path = self.root + "\\Pixiv_Crawler_Filter.json"
        if op.isfile(path):
            with open(path, 'r') as Cache_File:
                Filter_Dict = json.load(Cache_File)
        if not Filter_Dict:
            Filter_Dict = {"Illust_Date": 20180101, "Caption": "C[0-9]{1,2}|ã‚µãƒ³ãƒ—|åŽŸä½œ",
                           "Title" :"æŠ±ãæž•|å˜è¡Œæœ¬|ã‚³ãƒŸ|å“æ›¸|æ°¸é å¨˜|ä¾‹å¤§ç¥­|C[0-9]{1,2}|å®£ä¼|ç´¹ä»‹|ãŠçŸ¥ã‚‰ã›|ç·é›†ç·¨|å•†æ¥­å‘ŠçŸ¥|ã‚µãƒ³ãƒ—ãƒ«",
                           "Black_List": ["LINE stickers", "sticker", "creator's stickers", "oshinagaki", "manga", "comic", "Comike",
                                          "summer Comiket", "ç»ƒä¹ ", "å†¬CM", "è‰å›¾", "åˆ€å‰‘ä¹±èˆž", "è®²åº§", "æŠ±æž•", "notification", "å››æ ¼æ¼«ç”»",
                                          "æŠ±æž•å¥—", "Sunshine Creation", "ç»˜å›¾æ–¹æ³•", "ä½œç”»è¿‡ç¨‹", "è…å‘", "è½»å°è¯´", "æ±‡æ€»", "ç»˜å›¾æ–¹æ³•",
                                          "ã‚¢ã‚¤ãƒžã‚¹ã‚¯ãƒªãƒ¼ãƒãƒ£ãƒ¼"]}
            with open(self.root + "\\Pixiv_Crawler_Filter.json", 'w') as Cache_File:      
                json.dump(Filter_Dict, Cache_File)


        # åˆ©ç”¨æ­£åˆ™å¯¹ä½œå“æ ‡é¢˜è¿›è¡Œå¤„ç†
        Title = Illust_Dict["Title"].replace("|","")
        Title = Title.replace("\\","")
        Title = re.sub(r'[*]|[/]|[:]|[>]|[<]|["]',"",Title)
        Illust_Dict["Title"] = re.sub(r'[?]',"ï¼Ÿ",Title)
#         = re.sub(r'[(].{1,}[)]',"",Title)
        # æ ¹æ®å…³é”®è¯å†³å®šæ˜¯å¦ä¸‹è½½è¯¥å›¾ç‰‡
        Illust_Dict["is_block"] = False
        if int(Illust_Dict["Illust_Date"]) < Filter_Dict["Illust_Date"]:
            Illust_Dict["is_block"] = True
            return Illust_Dict
        if (not self.ReMatch("æ–°åˆŠè¡¨ç´™", Illust_Dict["Title"])) and self.ReMatch("æ–°åˆŠ", Illust_Dict["Title"]):
            Illust_Dict["is_block"] = True
            return Illust_Dict
        if self.ReMatch(Filter_Dict["Title"], Illust_Dict["Title"]):
            Illust_Dict["is_block"] = True
            return Illust_Dict
        if self.ReMatch(Filter_Dict["Caption"], Illust_Dict["Caption"]):
            Illust_Dict["is_block"] = True
            return Illust_Dict
        for Keyword in Filter_Dict["Black_List"]:
            if Tag.count(Keyword):
            # åŒ…å«é»‘åå•ä¸­tagçš„å›¾ç‰‡ç›´æŽ¥æŽ’é™¤
                Illust_Dict["is_block"] = True
                return Illust_Dict
        return Illust_Dict

    def Get_Illust_List(self, Artist_ID):
        if Artist_ID == "342929":
            print("è¯¥ç”¨æˆ·å·²æ³¨é”€")
            return 0
        # åˆ©ç”¨ç”¨æˆ·IDå‘Apiè¯·æ±‚ä½œè€…çš„ä¿¡æ¯
        Artist_Detail = self.api.user_detail(Artist_ID)
        if Artist_Detail:
            Name = self.Filter_Name(Artist_Detail.user.name)
            Total_Illusts = 30 if self.User_Info["Upadte_Mode"] else Artist_Detail.profile.total_illusts

            Catch_Name = "({}){}_Illust_List".format(Artist_ID, Name)
            Illust_List = self.Load_List(Catch_Name)
            # å¦‚æžœå‘çŽ°æ–°ä½œå“åˆ™é‡æ–°èŽ·å–ä¸€æ¬¡ä½œå“åˆ—è¡¨
            if Total_Illusts == len(Illust_List) - 2:
                for i in range(2,len(Illust_List)):
                    Illust_List[i] = self.Filter(Illust_List[i])
                self.Save_List(Catch_Name, Illust_List)
                print("æˆåŠŸä»Žç¼“å­˜ä¸­åŠ è½½ä½œè€… \033[32m({}){}\033[0m çš„ä½œå“åˆ—è¡¨ï¼Œå…±æœ‰ \033[33m{}\033[0m å‰¯ä½œå“ã€‚".format(Artist_ID, Name, Total_Illusts))
                return 1
            Illust_List = [Artist_ID, Name]

            # åˆ©ç”¨ç”¨æˆ·IDå‘Apiè¯·æ±‚ä½œå“åˆ—è¡¨
            json_result = self.api.user_illusts(Artist_ID)       
            while True:
                for Illust_Info in json_result.illusts:   
                    Illust_Dict = self.Collect_Illust_Info(Illust_Info)
                    Illust_List.append(Illust_Dict)
                self.Save_List(Catch_Name, Illust_List)
                print("\rä½œè€… \033[32m({}){}\033[0m çš„ä½œå“ä¿¡æ¯æ­£åœ¨æ”¶é›†ä¸­, è¿›åº¦ï¼š\033[33m{}/{}\033[0m".format(Artist_ID, Name,len(Illust_List) - 2, Total_Illusts), end="")
                if not json_result.next_url or self.User_Info["Upadte_Mode"]:
                    break
                # è¯·æ±‚æ¸…å•ä¸‹ä¸€é¡µ
                json_result = self.api.next_page_i(json_result.next_url)              
                    
            print("\rä½œè€… \033[32m({}){}\033[0m çš„å…¨éƒ¨ä½œå“æ”¶é›†å®Œæˆï¼Œå…±æœ‰ \033[33m{}\033[0m å‰¯ä½œå“ã€‚".format(Artist_ID, Name, Total_Illusts))
            return 1
        else:
            print("\æœªèŽ·å¾—IDä¸º \033[32m{}\033[0m çš„ä½œè€…ä¿¡æ¯ï¼Œè¯¥ç”¨æˆ·çš„è´¦å·å¯èƒ½å·²è¢«åœç”¨ã€‚ã€‚".format(Artist_ID))
            return 0


    def Get_Bookmark_List(self):
        Total_Illust_Bookmarks = self.api.user_detail(self.USER_ID).profile.total_illust_bookmarks_public
        # åˆ©ç”¨ç”¨æˆ·IDå‘Apiè¯·æ±‚æ”¶è—åˆ—è¡¨
        Catch_Name = "Bookmark_List"      
        Illust_List = self.Load_List(Catch_Name)
        # å¦‚æžœå‘çŽ°æ–°ä½œå“åˆ™é‡æ–°èŽ·å–ä¸€æ¬¡ä½œå“åˆ—è¡¨
        if Total_Illust_Bookmarks == len(Illust_List):
            print("æˆåŠŸä»Žç¼“å­˜ä¸­åŠ è½½ç”¨æˆ·çš„æ”¶è—åˆ—è¡¨ï¼Œå…±æœ‰ \033[33m{}\033[0m å‰¯ä½œå“ã€‚".format(Total_Illust_Bookmarks))
            return
        Illust_List = []

        # åˆ©ç”¨ç”¨æˆ·IDå‘Apiè¯·æ±‚æ”¶è—åˆ—è¡¨
        json_result = self.api.user_bookmarks_illust(self.USER_ID)       
        while True:
            for Illust_Info in json_result.illusts:
                Illust_Dict = self.Collect_Illust_Info(Illust_Info)
                Illust_List.append(Illust_Dict)
            self.Save_List(Catch_Name, Illust_List)
            print("ç”¨æˆ·çš„æ”¶è—åˆ—è¡¨æ­£åœ¨æ”¶é›†ä¸­, è¿›åº¦ï¼š\033[33m{}/{}\033[0m".format(len(Illust_List), Total_Illust_Bookmarks))
            if not json_result.next_url:
                break
            # è¯·æ±‚æ¸…å•ä¸‹ä¸€é¡µ
            json_result = self.api.next_page_b(json_result.next_url)              
                    
        print("ç”¨æˆ·çš„æ”¶è—åˆ—è¡¨æ”¶é›†å®Œæˆï¼Œå…±æœ‰ \033[33m{}\033[0m å‰¯ä½œå“ã€‚".format(Total_Illust_Bookmarks))

    def ReMatch(self, Re, str):
        if re.search(Re, str) is not None:
            return True
        else:
            return False

    def Filter_Name(self, Name):
        Name = re.sub(r'[@ï¼ /ðŸ”žâ– â—†].{1,}',"",Name)
        Name = re.sub(r'[Cc][0-9]{2,3}.{1,}',"",Name)
        Name = re.sub(r'[^a-zA-Z0-9\u4e00-\u9fa5]+-,\'()',"",Name)
        return Name

def main():
    crawler = Pixiv()
    download = Downloader(crawler.User_Info["Download_Path"])
    # ç”¨æˆ·ç•Œé¢
    while True:        
        print("\nåŠŸèƒ½ï¼š\n\t1. ä¸‹è½½å•ä¸ªä½œå“\n\t2. ä¸‹è½½æŒ‡å®šä½œè€…çš„ä½œå“\n\t3. ä¸‹è½½å…¨éƒ¨å·²æ”¶è—ä½œå“\n")
        print("\t4. æ”¶é›†ä¸ªäººå…³æ³¨åˆ—è¡¨\n\t5. ä¸‹è½½å…¨éƒ¨å·²å…³æ³¨ç”»å¸ˆçš„ä½œå“\n")
        print("\t6. è®¾ç½®ï¼ˆåˆæ¬¡ä½¿ç”¨è¯·è¿›å…¥è®¾ç½®ä¿®æ”¹ä¸‹è½½è·¯å¾„ï¼‰\n\t7. é€€å‡ºç¨‹åº")
        order = "2"
        order = input("è¯·è¾“å…¥æŒ‡ä»¤ï¼ˆ1-7ï¼‰ï¼š")
        os.system('cls')

        if order == "1":
            ID = input("è¯·è¾“å…¥ä½œå“IDï¼š")
            if int(ID) <= 0 :
                print("ä½œå“IDä¸æ­£ç¡®")
                continue
            Illust_Dict = crawler.Collect_Illust_Info("", ID = ID)
            Name = "({}){}".format(Illust_Dict["ID"], Illust_Dict["Title"])
            if Illust_Dict["Muti_Page"]:
                url = Illust_Dict["Image_Urls"]
                name = []
                file_name = "{}{}".format(Name, url[0][-4:])
                for i in range(len(url)):
                    name.append("{}_P{}{}".format(file_name[:-4], i, file_name[-4:]))     
                for i in range(5,len(url),5):
                    Threads = []
                    for j in range(5):
                        Threads.append(threading.Thread(target = download.Pic_Download, args = (url[i+j-5], name[i+j-5])))
                    for Thread in Threads:  Thread.start()
                    for Thread in Threads:  Thread.join()                                                       
                print("å›¾ç‰‡é›† {} ä¸‹è½½å®Œæˆ".format(file_name[:-4]))
            elif Illust_Dict["Ugoira"]:
                url = Illust_Dict["Image_Urls"][0]
                name = "{}@{}ms{}".format(Name, Illust_Dict["Image_Urls"][1], url[-4:])
                download.Gif_Threads_Pool.append(threading.Thread(target = download.Gif_Download, args = (url, name)))
                download.Gif_Threads_Pool[-1].start()
                if len(download.Gif_Threads_Pool) == 10:
                    for Thread in download.Gif_Threads_Pool:     Thread.join()
                    download.Gif_Threads_Pool = []
                #download.Gif_Download(url, name)
            else:
                url = Illust_Dict["Image_Urls"]
                name = "{}{}".format(Name, url[-4:])
                download.Pic_Download(url, name)
            

        elif order == "2":
            ID = "50258193"
            ID = input("è¯·è¾“å…¥ä½œè€…IDï¼š")
            if int(ID) <= 0 :
                print("ä½œè€…IDä¸æ­£ç¡®")
                continue
            crawler.Get_Illust_List(ID)
            download.Artist_Download(ID)

        elif order == "3":
            crawler.Get_Bookmark_List()
            download.Bookemark_Download()

        elif order == "4":
            crawler.Get_Follow_List()
            print("æ”¶é›†å®Œæˆï¼Œç¼“å­˜æ–‡ä»¶ä½äºŽ{}\\cache\\æ–‡ä»¶å¤¹ä¸‹ï¼Œè¯·ä¸è¦åˆ é™¤ã€‚".format(crawler.root))

        elif order == "5":
            crawler.Get_Follow_List()
            # å¦‚æžœå­˜åœ¨ç¼“å­˜æ–‡ä»¶ï¼Œåˆ™ä»Žæœ¬åœ°è¯»å–è¯¥ç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨
            Follow_List = crawler.Load_List("{}_Follow_List".format(crawler.USER_NAME))
            for artist in Follow_List[crawler.User_Info["Collect_Progress"]:]:
                result = crawler.Get_Illust_List(artist[0])
                if result:
                    download.Artist_Download(artist[0])
                crawler.User_Info["Collect_Progress"] += 1
                crawler.Save_List("User_Info", crawler.User_Info)
                print("\n{}ä¸ªç”»å¸ˆå·²ä¸‹è½½ï¼Œå‰©ä½™{}ä¸ª\n".format(crawler.User_Info["Collect_Progress"], len(Follow_List) - crawler.User_Info["Collect_Progress"]))
            crawler.User_Info["Collect_Progress"] = 0
            crawler.Save_List("User_Info", crawler.User_Info)            
            print("å…¨éƒ¨ä¸‹è½½å®Œæˆ")

        elif order == "6":
            while True:
                print("1. ç™»é™†é‚®ç®±\n2. å¯†ç \n3. ä¸‹è½½è·¯å¾„\n4. å˜æ›´ä¸‹è½½æ¨¡å¼\n5. é€€å‡º")
                order = input("è¯·è¾“å…¥æŒ‡ä»¤ï¼ˆ1-4ï¼‰ï¼š")
                if order == "1":
                    crawler.User_Info["User_Name"] = input("è¯·è¾“å…¥ï¼š\n")
                    crawler.Save_List("User_Info", crawler.User_Info)
                elif  order == "2":
                    crawler.User_Info["Password"] = self.RSA.encrypt(input("è¯·è¾“å…¥ï¼š\n"))
                    crawler.Save_List("User_Info", crawler.User_Info)
                elif  order == "3":
                    print("å½“å‰ä¸‹è½½è·¯å¾„ä¸ºï¼š {}".format(crawler.User_Info["Download_Path"]))
                    Path = input("è¯·è¾“å…¥ï¼š\n")
                    if Path:
                        if Path[-1] != "\\":
                            Path += "\\"
                        crawler.User_Info["Download_Path"] =  Path
                        download.Set_Path(Path)
                        crawler.Save_List("User_Info", crawler.User_Info)
                    print("å½“å‰ä¸‹è½½è·¯å¾„ä¸ºï¼š {}\n".format(crawler.User_Info["Download_Path"]))
                elif  order == "4":
                    print("å½“å‰ä¸‹è½½æ¨¡å¼ä¸ºï¼š {}".format("å¢žé‡æ›´æ–°" if crawler.User_Info["Upadte_Mode"] else "å…¨éƒ¨ä¸‹è½½"))
                    if input("æ˜¯å¦è¿›è¡Œå˜æ›´ï¼Ÿ(y/n)\n") == "y" :
                        crawler.User_Info["Upadte_Mode"] = not crawler.User_Info["Upadte_Mode"]
                        crawler.Save_List("User_Info", crawler.User_Info)
                        print("çŽ°åœ¨çš„ä¸‹è½½æ¨¡å¼ä¸ºï¼š {}".format("å¢žé‡æ›´æ–°" if crawler.User_Info["Upadte_Mode"] else "å…¨éƒ¨ä¸‹è½½"))
                elif  order == "5":
                    break

        elif order == "7":
            crawler.Save_List("User_Info", crawler.User_Info)
            break

if __name__ == '__main__':
    main()